$ sudo apt-get update
$ sudo ntpdate pool.ntp.org
$ sudo apt-get install ntp
$ sudo apt-get install python-software-properties

download and install Apache Spark
----------------------------------
$ cd ~
$ mkdir spark
$ cd spark/
$ wget http://mirror.tcpdiag.net/apache/spark/spark-1.6.0/spark-1.6.0.tgz
$ gunzip -c spark-1.6.0.tgz | tar -xvf -
$ cd spark-1.6.0/
$ sudo sbt/sbt assembly
$ cd ..
$ sudo cp -Rp spark-1.6.0 /usr/local/
$ cd /usr/local/
$ sudo ln -s spark-1.6.0 spark

create a Spark user with proper privileges and ssh keys.
=========================================================
$ sudo addgroup spark
$ sudo useradd -g spark spark
$ sudo adduser spark sudo
$ sudo mkdir /home/spark
$ sudo chown spark:spark /home/spark

Add to /etc/sudoers file:
=================================
spark ALL=(ALL) NOPASSWD:ALL

$ sudo chown -R spark:spark /usr/local/spark/
$ sudo su spark
$ ssh-keygen -t rsa -P ""
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
$ ssh localhost
$ exit

setup some Apache Spark working directories with proper user permissions
===========================================================================
$ sudo mkdir -p /srv/spark/{logs,work,tmp,pids}
$ mkdir /tmp/spark-events
$ sudo chown -R spark:spark /srv/spark
$ sudo chmod 4755 /srv/spark/tmp


let's adjust some Spark configuration files
===========================================
$ cd /usr/local/spark/conf/
$ cp -p spark-env.sh.template spark-env.sh
$ vim spark-env.sh

SPARK-ENV.SH (ADD BELOW)
#Make sure to put your change SPARK_PUBLIC_DNS=“PUBLIC IP“ to your Public IP

export SPARK_WORKER_CORES="2"
export SPARK_WORKER_MEMORY="1g"
export SPARK_DRIVER_MEMORY="1g"
export SPARK_REPL_MEM="2g"
export SPARK_WORKER_PORT=9000
export SPARK_CONF_DIR="/usr/local/spark/conf"
export SPARK_TMP_DIR="/srv/spark/tmp"
export SPARK_PID_DIR="/srv/spark/pids"
export SPARK_LOG_DIR="/srv/spark/logs"
export SPARK_WORKER_DIR="/srv/spark/work"
export SPARK_LOCAL_DIRS="/srv/spark/tmp"
export SPARK_COMMON_OPTS="$SPARK_COMMON_OPTS -Dspark.kryoserializer.buffer.mb=32 "
LOG4J="-Dlog4j.configuration=file://$SPARK_CONF_DIR/log4j.properties"
export SPARK_MASTER_OPTS=" $LOG4J -Dspark.log.file=/srv/spark/logs/master.log "
export SPARK_WORKER_OPTS=" $LOG4J -Dspark.log.file=/srv/spark/logs/worker.log "
export SPARK_EXECUTOR_OPTS=" $LOG4J -Djava.io.tmpdir=/srv/spark/tmp/executor "
export SPARK_REPL_OPTS=" -Djava.io.tmpdir=/srv/spark/tmp/repl/\$USER "
export SPARK_APP_OPTS=" -Djava.io.tmpdir=/srv/spark/tmp/app/\$USER "
export PYSPARK_PYTHON="/usr/bin/python"
SPARK_PUBLIC_DNS="PUBLIC IP"
export SPARK_WORKER_INSTANCES=1


$ cp -p spark-defaults.conf.template spark-defaults.conf
$ vim spark-defaults.conf


SPARK-DEFAULTS (ADD BELOW)
#Make sure to put your change hostnamepub to your Public IP

spark.master            spark://hostnamepub:7077
spark.executor.memory   512m
spark.eventLog.enabled  true
spark.serializer        org.apache.spark.serializer.KryoSerializer
